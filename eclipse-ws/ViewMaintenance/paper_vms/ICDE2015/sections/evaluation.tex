\section{Evaluation}
\label{sec:evaluation}

In this section, we report on the results of an extensive experimental
evaluation of our approach.  We fully implemented VMS in Java and
integrated it with Apache HBase. Before we discuss our experiments, we
briefly review the experimental set-up and the generated workload.

%
% aj - this section - if we can afford it space-wise, it would be better to
%                              use 3 subsections for Impl., Setup, and Experiments
%\subsection{Implementation} 
%The integration of VMS and HBase involves
%two simple steps.  First, we integrate the NX component in the HBase
%architecture. Second, we let the VM component interact with HBase via
%its client API. Figure~\ref{fig:system_overview} shows the
%integration.
%
%In HBase parlance a \textit{region server} (i.e., a node) writes
%client updates to a TL (HBase calls it write-ahead log), which the NX
%simply reads via the file system API, as described in
%Section~\ref{sec:kv_model}.  HBase uses the HDFS file system, where TL
%and table files are persisted for every node.  We use the HBase
%internal class \texttt{HLog.Reader} to interpret the log content and
%retrieve entries in a readable form, as they are serialized and not
%stored in plain text.  NX extracts the row key and routes the
%operation to the appropriate VM. A single TL entry in HBase contains
%exactly the information VMS expects, i.e., row key, table name,
%etc. The difficulties of not being able to differentiate inserts from
%updates and not having access to old column values via the log are the
%as in our model.  An instance of NX is deployed with every node.
%
%As a VM starts processing operations and materializing views, it
%connects to the HBase API to submit view table updates as prescribed
%by our design.  A VM uses an abstract class \texttt{KVSTableService},
%which contains all methods required to access the KV-store and to
%materialize each view type. For each view type, these methods are:
%\texttt{put()}, \texttt{get()}, \texttt{delete()},
%\texttt{checkAndPut()} and \texttt{checkAndDelete()}. Now, the VM is
%able to access HBase and update view tables.
%
%As can be seen from this discussion VMS and the underlying KV-store
%are neatly decoupled, requiring only two integration points (client
%API and TL).

%
% aj - below - can we justify the setup a bit; why that distribution of nodes?
%                   - did we try out other distributions and this was the best?
% This is minor.
%

\subsection{Experimental set-up}  
All experiments were performed on a
cluster comprised of 40 nodes (running Ubuntu 14.04). Out of these, 11
machines were dedicated to the Apache Hadoop (v1.2.1) installation,
one machine as name node (HDFS master) and 10 machines as data nodes
(HDFS file system). On HDFS, we installed HBase (v0.98.6.1) with one
master and 10 region servers. On every region server, we deployed one 
of our extensions (cf. Figure~\ref{fig:kv_model}). View managers (VMs) 
were deployed on 20 separate
machines to be able to scale them without interfering with the core
system (i.e., the 12 HBase nodes.) Also, running multiple VMs on the
same machine is possible, since, as we observed, the limiting factor
for a VM is the access latency to HBase (i.e., request processing
latency).  Finally, another 8 nodes were reserved for HBase clients to
simulate the load on the base tables by constantly issuing updates.

\subsection{Workload} 
Prior to each experiment, we created an empty
base table and a set of view tables. Concrete view definitions as well
as the assignment of view tables to base tables are maintained as meta
data in a separate \textit{view definition} table. By default, HBase
stores all base table records in one region. We configured
HBase to split every table into 50 parts -- which lets HBase balance 
the regions with high granularity and ensures an uniform
distribution of keys among available region servers. 

\begin{figure*}
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/selection}
      \vspace{-5mm}
  \caption{Selection performance}\label{fig:selection}
        \vspace{-5mm}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/aggregations}
      \vspace{-5mm}
  \caption{Aggregation performance}\label{fig:viewtypes}
        \vspace{-5mm}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{figures/join}
      \vspace{-5mm}
  \caption{Join performance}\label{fig:join}
        \vspace{-5mm}
\endminipage
\end{figure*}

For \texttt{COUNT}, \texttt{SUM}, \texttt{MIN}, and \texttt{MAX}
views, we create base tables that contain one column $c_1$
(aggregation key) and another column $c_2$ (aggregation value).  We
choose a random number $r_a$ between $1$ and some upper bound $U$ to
generate aggregation keys.  Thereby, it is possible to vary the number
of base table records that affect one particular view table
record. For the \texttt{SELECTION} view, we use the same base table
layout and apply the selection condition to column $c_2$. The
\texttt{JOIN} view requires two base tables with different row
keys. The row key of the right table is stored in a column in the left
table, referred to as foreign key.

Basically, the workload we generate for our experiments consists of
\textit{insert}, \textit{update} and \textit{delete} operations that
are issued to HBase using its client API. Operations are generated
according to different distributions over the key space (we use Zipf
and Uniform).  A Zipf distribution, for instance, simulates a ``hot
data'' scenario, where only few base table records are updated very
frequently.


\subsection{Experiments} 

With our experiments, we primarily evaluate the performance of the
system with regards to throughput and view maintenance latency.

\subsubsection{Impact of view type on throughput} 
First, we evaluate the
performance for every view type, separately. We measure the throughput
during view maintenance, i.e., the number of updates a view can
sustain per second.  We configure a system with a fixed number of 10
region servers to host all tables. The number of VMs varies between 10
to 50, and all VMs are assigned evenly to region servers. Furthermore,
40 clients generate a total of 1 million operations per
experiment. Updates are concurrently processed by all VMs. The
experiments are completed after all clients sent their updates and all
VMs emptied their queues.

For the \texttt{SELECTION} view, we experimented with three different
selectivity values(i.e., selection of 10, 50 and 90 percent of base
records), while varying the number of VMs that process the update
load. Figure~\ref{fig:selection} shows the results.  The
\texttt{SELECTION} view is not realized with an auxiliary
view. Compared to the other views, its maintenance results in the
highest throughput.  The performance depends on the amount of records
selected. Interesting is that the absolute throughput is limited by
the throughput clients exert on the system. In
Figure~\ref{fig:selection}, the performance is monotonically decreasing
for a selectivity of 10\%. This means, VMs propagate updates as fast
as they are applied to HBase by clients. Increasing the number of view
managers only slows down the system as more components are running
concurrently.

Figure~\ref{fig:viewtypes} shows the performance of the aggregation
view types: \texttt{COUNT}, \texttt{SUM}, \texttt{MIN}, \texttt{MAX},
and \texttt{INDEX}. Again, we use a fixed configuration comprised of
10 region servers and 40 clients that generated 1M operations.
Aggregation views derive from an auxiliary view (here a \texttt{DELTA}
view.)  Therefore, the throughput for aggregation views is lower than
for the \texttt{SELECTION} view. However, in contrast to the
\texttt{SELECTION} view, the throughput significantly benefits from
increasing the number of VMs. Not surprisingly, \texttt{COUNT} and
\texttt{SUM} views show similar performance, as their maintenance is
nearly identical.  \texttt{INDEX} view maintenance exhibits a lower
throughput than \texttt{COUNT} and \texttt{SUM}, which only store one
attribute for the aggregated value, whereas the \texttt{INDEX} view
stores a variable number of primary keys associated with the index
column value of the indexed table.  The performance of both
\texttt{MIN} and \texttt{MAX} is worse compared to the \texttt{INDEX}
view. In a \texttt{MIN} (max) view, we also store base table records,
together with the aggregated value.  The storage overhead is equal to
\texttt{INDEX}, but sometimes the values of an entire row needs to be
queried to recalculate the new minimum (maximum).  


Figure~\ref{fig:join} shows the evaluation results for the
\texttt{JOIN} view under the same conditions as above. Compared to the
other view types and not surprising, the \texttt{JOIN} shows lower
throughput.  The \texttt{JOIN} view requires a more complex internal
auxiliary view table constellation and maintenance. However, we suspect
that throughput is still higher than as if full table scans would have
to be used to find matching rows (not considering consistency issues,
if scans would be used.)  Similar to aggregation views, \texttt{JOIN}
benefits from an increased number of view managers. Also, here as
well, auxiliary tables for \texttt{JOIN} can be amortized as more
views are defined.

\begin{figure*}
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/benefit}
      \vspace{-5mm}
  \caption{Benefit of view maintenance}\label{fig:benefits}
        \vspace{-3mm}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/scale_rs}
      \vspace{-5mm}
  \caption{Scale region servers}\label{fig:regionservers}
        \vspace{-3mm}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{figures/zipf}
      \vspace{-5mm}
  \caption{Zipf distribution}\label{fig:zipf}
        \vspace{-3mm}
\endminipage
\end{figure*}
\begin{figure*}
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/cost}
      \vspace{-5mm}
  \caption{Cost of view maintenance}\label{fig:cost}
      \vspace{-5mm}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/scale_views}
      \vspace{-5mm}
  \caption{Scale views}\label{fig:views}
      \vspace{-5mm}
\endminipage\hfill
\minipage{0.32\textwidth}%
  \includegraphics[width=\linewidth]{figures/killvm}
      \vspace{-5mm}
  \caption{Fault tolerance - Impact of crash}\label{fig:killvm}
      \vspace{-5mm}
\endminipage
\end{figure*}


\subsubsection{Costs and benefit of view maintenance} 

To determine the
benefits, i.e., the latency improvement a client experiences when
accessing a view, and the costs, i.e., essentially, the decrease in
overall system throughput that results, we conducted an experiment
with three different view maintenance strategies: (i) \textit{Client
  table scan:} To obtain the most recent count view values, a client
scans the base table and aggregates all values on its own. (ii)
\textit{Server table scan:} To obtain the most recent count view
values, the client sends a request to HBase, which internally computes
the view in a custom manner and returns it as output. In the
implementation, we use HBase's ability to parallelize table access.
All region servers scan their part of the table and, at the end,
intermediate results are collected and merged.  (iii)
\textit{Materialized view:} Views are maintain incrementally by VMS
configured with 40 VMs used to materialize the count view in parallel.
For (i) and (ii), we disregard inconsistencies that may result from
concurrent table updates while scans are in progress. Our primary
objective is to obtain some understanding of how VMS fares relative to
other potential approaches.

The results are given in Figure~\ref{fig:benefits}, where we measure
the latency a client perceives (i.e., the time to wait for the result)
as the scanned key range increases.  The first strategy performed
worst. \textit{Client table scans} are sequential by nature and
require a large number of RPCs to HBase, even if requests are
batched. Especially with increasing table size, this approach becomes
more and more impracticable.

\textit{Server table scan} is promising because HBase is able to
exploit the data distribution, which results in a linear speed-up.
Nonetheless, the time to obtain the most recent values for a count
aggregate reached 5~s for a table with 1M rows.  While this result
could now be cached in materialized form, in scenarios with frequently
changing base data, recalculations would have to be frequently
repeated to keep the results up-to-date. Moreover, concurrent table
scans may interfere with the write performance of region servers.

Because views are materialized and updates are done incrementally,
latencies for the third strategy are exactly the same as for every
HBase table. Moreover, the client only accesses the aggregated values
and not the base table. Therefore, the latency remains below 1~ms,
even for a base table with 10 million rows.

Materializing views comes at a cost. We assess this cost as the
performance impact on base table creation time (here, defines as the
time to insert 1M tuples into the base table.)  Figure~\ref{fig:cost}
shows the base table creating time with and without concurrent view
maintenance.  We track creation time as the number of clients
increases.  Figure~\ref{fig:cost} shows view maintenance with 20, 30
and 40 VMs.  While view maintenance increases the base table creation
time by approximately 40\%, a further increase of VMs (by ten) only
increases base table creation time by approximately 2\%.


\subsubsection{System scalability}  
In Figure~\ref{fig:selection} and
Figure~\ref{fig:viewtypes}, we evaluated the performance of different
view types. In both experiments, we increased the number of VMs.  Now,
we examine system performance when scaling up region servers. In
Figure~\ref{fig:regionservers}, the number of region servers is varied
from 4 to 10. We run the experiment with a selection, a count and an
index view and measure throughput. We observe an almost linear
increase of throughput independent of the view type processed.

The effect can be explained as follows: Each region server runs on a
separate node. Adding another region server results in additional I/O
channels (i.e., separate disk).  Thus, the overall performance of
HBase increases. Clients requests are completed faster due to the
additional I/O capacity.  Likewise, VMs can perform faster view
updates.  We draw the following conclusion: Scaling up the number of
VMs as in Figure~\ref{fig:selection} and Figure~\ref{fig:viewtypes}
improves the view maintenance throughput up to a point where VMs are
saturated with updates that they can push through the available I/O
channels.  Scaling up the region server improves overall performance,
also for VMs, due to the additional I/O capacity.

Figure~\ref{fig:views} shows system throughput as multiple views are
maintained by a varying number of VMs.  We note a performance leap as
load changes from maintaining one to two count views. In our workload
the count views derive from the same auxiliary table, which must be
maintained as well, but only once. In further increasing the number of
views, the effect diminishes.  All aggregation views, especially join
views, benefit from the sharing of auxiliary views.

However, increasing the number of views in the system, also increases
the lag between base table states and derived view table
states. While, as we show in Section~\ref{sec:consistency}, our view
states are consistent, they do lag behind in time.  This lag can be
reduced by increasing the number of VMs to speed up view maintenance.
Thus, the more views we maintain, the more important is the number of
VMs allocated.

\subsubsection{Impact of data distribution}  

Figure~\ref{fig:zipf}
evaluates the effect of different distributions on system performance.
We scale the number of VMs and track the latency for creating base
tables and derived views.  Keys of update operations are drawn from a
Zipf and a Uniform distribution.

For the Uniform distribution, creation latencies are independent of
the number of VMs assigned, even small numbers of VMs can handle the
load in our set-up.  For the Zipf distribution, latencies are much
higher. We also see that creation and maintenance latency increase,
yet are positively effected by increasing the number of VMs that
propagate updates.  Thus, especially for a skewed workload, the system
greatly benefits from being able to dynamically assign VMs.  VMs can
be assigned to hot spots in the key range, away from ranges where they
are not needed.  Nevertheless, in this case, HBase may constitute a
bottleneck for clients. Clients issue updates to a set number of
region servers that handle 90\% of the load, thus, resulting in a
slowdown.  HBase developers suggest that keys of updates should be
salted. That is a prefix is assigned to keys, such that their
distribution becomes uniform again. In this case, VMs propagate
updates as we saw above.


\subsubsection{Impact of VM crash}  
Figure~\ref{fig:killvm} shows the
impact of a view manager crash on system performance.  In this
experiment, we track view maintenance latency, as the number of VMs
available in the system crashes.  We ran experiments with 20, 30 and
40 VMs, respectively. Twenty seconds after view maintenance started,
we terminate a number of VMs. In this experiment, the VMs are
distributed evenly to region servers. In a set-up with 20 VMs and 10
region servers, we have a ratio of 2 VMs per region
server. Terminating both VMs of the same region server stops view
maintenance. The operations arriving at that region server cannot be
forwarded until a new VM is assigned to the region server. In the
experiment, we terminate VMs of different region servers.

In our set-up, a single crashing VM reduces the view maintenance
latency, because it takes additional time for recovery to replay the
TL and because the remaining VMs have to absorb additional load.  The
failing of further VMs does not further impact the situation.  As long
as every region server loses only one of its two VMs, the overall
processing time remain the same.  Overall, the impact of a VM crash
lessens, the more VMs are deployed.  Thus, increasing the number of
VMs, increases the fault resilience of the system.

